# -*- coding: utf-8 -*-
"""upsampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SgMWk1wKR-qyyoLDp72e-7Tk9S_FptPN
"""

from scipy.io import wavfile
from scipy.fftpack import fft
import matplotlib.pyplot as plt
import numpy as np
import IPython.display as ipd
import random

import tensorflow as tf

from random import shuffle

import subprocess
import io
import os

SAMPLE_RATE = 48000
DOWNSAMPLE_RATIO = 2
AUDIO_SIZE = 5000
LARGEST_INT16 = abs(np.iinfo('int16').min)
LARGEST_UINT16 = np.iinfo('uint16').max
DIRECTORY = '/content/drive/My Drive/songs/'
MODEL = '/content/drive/My Drive/AudioNet/'

  
def downsample(audio_left, audio_right):
  size_l, size_r = len(audio_left), len(audio_right)
  downsample_right = np.array([audio_right[x] for x in range(0, size_l, DOWNSAMPLE_RATIO)], dtype=np.int_)
  downsample_left = np.array([audio_left[y] for y in range(0, size_r, DOWNSAMPLE_RATIO)], dtype=np.int_)
  return downsample_left, downsample_right

  
def norm(samples):
  d = np.array([x + LARGEST_INT16 for x in samples], dtype=np.int_)
  return d.astype(np.float32) / LARGEST_UINT16

  
def trim_audio(original_audio, size=100):
  try:
      length = original_audio.shape[0]
      assert length >= 100000, "Bad dataset, audio file too short"
      start = random.randint(0, length - size)
      trimmed = original_audio[start:start+size]
      left_audio = [s[0] for s in trimmed]
      right_audio = [s[1] for s in trimmed]
      return left_audio, right_audio, True
  except:
      print("Unusable data, moving to next in set")
      return [], [], False

def downsample_audio(samples_per_file): 
  original_audio, audio_samples = [], []
  direct = list(os.listdir(DIRECTORY))
  shuffle(direct)
  for file in direct:
      filename = os.fsdecode(file)
      if filename.endswith(".wav"):
          for _ in range(samples_per_file):
              rate, signal = wavfile.read(DIRECTORY+filename)
              trimmed_left, trimmed_right, usable = trim_audio(signal, size=AUDIO_SIZE)
              if usable and rate > 40000:
                downsampled_left, downsampled_right = downsample(trimmed_left, trimmed_right)
                original_audio.append(norm(np.expand_dims(trimmed_left, axis=2)))
                original_audio.append(norm(np.expand_dims(trimmed_right, axis=2)))
                audio_samples.append(norm(np.expand_dims(downsampled_left, axis=2)))
                audio_samples.append(norm(np.expand_dims(downsampled_right, axis=2)))
      else:
          continue
  original_audio = np.array(original_audio, dtype=np.float32)
  audio_samples = np.array(audio_samples, dtype=np.float32)
  return original_audio, audio_samples

  
def fit_gen(samples, original, batch_size=4, valid=False, valid_size=20):
    assert batch_size % 4 == 0
    count, samples_count = 0, 0
    audio_clip, downsamples = np.zeros((batch_size, AUDIO_SIZE)), np.zeros((batch_size, AUDIO_SIZE//DOWNSAMPLE_RATIO, 1))
    while True:
      if valid:
        indecies = np.random.randint(len(samples)-valid_size, len(samples))
      else:
        indecies = np.random.randint(0, len(samples)-valid_size)
      if count == batch_size:
        yield downsamples, audio_clip
        count = 0
        audio_clip, downsamples = np.zeros((batch_size, AUDIO_SIZE)), np.zeros((batch_size, AUDIO_SIZE//DOWNSAMPLE_RATIO, 1))
      else:
        audio_clip[count] = original[indecies].flatten()
        downsamples[count] = samples[indecies]
        count += 1

# class SubpixelShuffle(layers.Layer):
#     def __init__(self, out_length, **kwargs):
#         super(SubpixelShuffle, self).__init__(**kwargs)
#         self.out_length = out_length
        
#     def call(self, inputs, training=None):
#         def f(inp):
#             return subpixel_shuffle(inp, (self.out_length, None))
#         return tf.map_fn(f, inputs)
    
#     def compute_output_shape(self, input_shape):
#         return (None, self.out_length, int(input_shape[2]//(self.out_length/input_shape[1])))


class SubpixelShuffle(tf.keras.layers.Layer):
    def __init__(self, r, **kwargs):
        super(SubpixelShuffle, self).__init__(**kwargs)
        self.r = r
        
    def call(self, I, training=None):
        _, w, rc = I.shape
        r = self.r
        assert rc % r == 0
        c = rc // r
        X = tf.transpose(I, [2,1,0]) # (rc, w, b)
        X = tf.batch_to_space_nd(X, [r], [[0,0]]) # (c, r*w, b)
        X = tf.transpose(X, [2,1,0])
        return X
        
        return tf.map_fn(
            lambda inp: tf.contrib.periodic_resample.periodic_resample(
                inp, (inp.shape[0] * self.r, None)), inputs)
    
    def compute_output_shape(self, input_shape):
        t, n_filter = input_shape
        return (None, t*r, n_filter//r)

input_size = AUDIO_SIZE // DOWNSAMPLE_RATIO

inputs = tf.keras.Input(shape=(input_size, 1), name='audiofiles')
# bottleneck
x = tf.keras.layers.Conv1D(DOWNSAMPLE_RATIO * 100, input_size//50, strides=2, 
                           data_format="channels_last", padding="same")(inputs)
x = tf.keras.layers.BatchNormalization(axis=-1)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LeakyReLU()(x)
# upsample
x = tf.keras.layers.Conv1D(DOWNSAMPLE_RATIO * 100, input_size//50, strides=1, 
                           data_format="channels_last", padding="same")(x)
x = tf.keras.layers.BatchNormalization(axis=-1)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.LeakyReLU()(x)
# x = SubpixelShuffle(inputs.shape[1]//2)(x)
x = SubpixelShuffle(DOWNSAMPLE_RATIO)(x)
# final conv
x = tf.keras.layers.Conv1D(2, input_size//50, strides=1, 
                           data_format="channels_last", padding="same")(x)
x = tf.keras.layers.LeakyReLU()(x)
# outputs = SubpixelShuffle(inputs.shape[1]*2)(x)
outputs = SubpixelShuffle(DOWNSAMPLE_RATIO//2)(x)
outputs = tf.keras.layers.Flatten()(outputs)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.summary()

model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='mse',
              metrics=['mae', 'mse'])

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive/')

checkpoint_path = MODEL + "audio.h5"

if os.path.getsize(checkpoint_path) > 0:
  model = tf.keras.models.load_model(checkpoint_path, 
                                     custom_objects=
                                     {'SubpixelShuffle': tf.keras.layers.Layer})
  model.summary()
  model.compile(optimizer=tf.train.AdamOptimizer(), 
                loss='mse', 
                metrics=['mae', 'mse'])

# from keras import models

# def freeze(model):
#     for layer in model.layers:
#         layer.trainable = False

#         if isinstance(layer, models.Model):
#             freeze(layer)

SAMPLES_PER_FILE = 40
BATCH_SIZE = 8
num_files = len(os.listdir(DIRECTORY))-1
original, samples = downsample_audio(SAMPLES_PER_FILE)
inputs = fit_gen(samples, original, batch_size=BATCH_SIZE, valid_size=120)
valid = fit_gen(samples, original, batch_size=BATCH_SIZE, valid=True, valid_size=120)
num_inputs = SAMPLES_PER_FILE * num_files * 2
model.fit_generator(inputs, validation_data=valid, validation_steps=15,
                    steps_per_epoch=num_inputs//BATCH_SIZE, epochs=1)
# freeze(model)
tf.keras.models.save_model(model, checkpoint_path, overwrite=True)
# model.save(checkpoint_path)

OUTPUTFILES = '/content/drive/My Drive/reconstructed audio/'

def get_prediction_audio(audio_size, left=True):
    direct = list(os.listdir(DIRECTORY))
    shuffle(direct)
    for file in direct:
      filename = os.fsdecode(file)
      if filename.endswith(".wav"):
          rate, signal = wavfile.read(DIRECTORY+filename)
          left, right, usable = trim_audio(signal, size=audio_size)
          if usable:
            break
      else:
          continue
    if left:
      original_audio = np.array(left, dtype=np.float32)
      audio_samples = np.array(left, dtype=np.float32)
    else:
      original_audio = np.array(right, dtype=np.float32)
      audio_samples = np.array(right, dtype=np.float32)
    return original_audio, audio_samples
  
  
def splice_audio(samples, cut=6000):
  size = len(samples)
  return np.array([samples[x:x+cut] for x in range(0, size, cut)])


def downsample_1D(audio, ds=2):
  size = len(audio)
  return np.array([audio[x] for x in range(0, size, ds)], dtype=np.int_)


def run_predictions(all_clips):
  predicted = []
  for x in all_clips:
    to_predict = np.array([norm(np.expand_dims(x, axis=2))], dtype=np.float32)
    predict = model.predict(to_predict)
    predicted.append(predict[0])
  return np.array(predicted, dtype=np.float32)

original, to_ds = get_prediction_audio(120000)
orig_ds = downsample_1D(original, ds=2)
split_ds = np.array([downsample_1D(x, ds=DOWNSAMPLE_RATIO) for x in splice_audio(to_ds, cut=AUDIO_SIZE)], dtype=np.int_)
predictions = run_predictions(split_ds)
predictions = predictions.flatten()
predictions = predictions.astype(np.float32) * LARGEST_UINT16 - LARGEST_INT16
predictions = predictions.astype(np.int16)

plt.plot(orig_ds)
print(max(orig_ds), min(orig_ds))
ipd.Audio(orig_ds, rate=48000/2)

plt.plot(original)
print(max(original), min(original))
ipd.Audio(original, rate=48000)

plt.plot(predictions)
print(max(predictions), min(predictions))
ipd.Audio(predictions, rate=48000*2)

